{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000313/data/20240731021525/data.tar.gz"
      ],
      "metadata": {
        "id": "PttNWjcSq_Pw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf data.tar.gz"
      ],
      "metadata": {
        "id": "MYw6CL4erAPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnbhpwkgq70q"
      },
      "source": [
        "### Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeKs8aQ2q70w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import lightgbm as lgb\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uifuKyaq701"
      },
      "source": [
        "### Data Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzOyqR5Uq702"
      },
      "outputs": [],
      "source": [
        "# 파일 호출\n",
        "data_path: str = \"./data\"\n",
        "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train\n",
        "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
        "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
        "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-P5OT8Rq703"
      },
      "outputs": [],
      "source": [
        "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
        "file_names: List[str] = [\n",
        "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
        "]\n",
        "\n",
        "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
        "file_dict: Dict[str, pd.DataFrame] = {\n",
        "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
        "}\n",
        "\n",
        "for _file_name, _df in tqdm(file_dict.items()):\n",
        "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
        "    _rename_rule = {\n",
        "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
        "        for col in _df.columns\n",
        "    }\n",
        "    _df = _df.rename(_rename_rule, axis=1)\n",
        "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(axis=1, how='all')"
      ],
      "metadata": {
        "id": "gPDe6uR29Zk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_67zGyjq71E"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-igms0vq71E"
      },
      "outputs": [],
      "source": [
        "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
        "cols_dict: Dict[str, str] = {\n",
        "    \"ID\": \"ID\",\n",
        "    \"target\": \"target\",\n",
        "    \"_type\": \"_type\",\n",
        "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
        "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
        "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
        "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
        "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
        "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
        "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
        "    \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
        "    \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
        "    \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
        "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
        "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
        "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
        "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
        "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
        "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
        "}\n",
        "\n",
        "erased_dict: Dict[str, str] = {\n",
        "}\n",
        "\n",
        "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yUOJW2uq71F"
      },
      "outputs": [],
      "source": [
        "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
        "df = df.assign(\n",
        "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
        "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
        "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
        "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
        "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
        "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
        "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
        ")\n",
        "'''erased_df = {\n",
        "\n",
        "}'''\n",
        "# category, continuous 열을 따로 할당해둠\n",
        "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\"]\n",
        "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
        "    \"liquidation_diff\",\n",
        "    \"buy_sell_volume_ratio\",\n",
        "    \"liquidation_usd_diff\",\n",
        "    \"volume_diff\",\n",
        "]\n",
        "'''\n",
        "erased_conti_cols = {\n",
        "}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract numerical features for correlation analysis\n",
        "numerical_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation matrix using Spearman method on numerical features\n",
        "corr_matrix = numerical_df.corr(method='spearman')\n",
        "\n",
        "corr_with_target = corr_matrix['target'].abs().sort_values(ascending=False)\n",
        "print(\"Top correlated features:\\n\", corr_with_target.head(10))\n",
        "\n",
        "'''# Get column names and extract the last three words separated by '_'\n",
        "column_names = corr_matrix.columns\n",
        "last_three_words = ['_'.join(name.split('_')[-3:]) for name in column_names]\n",
        "\n",
        "# Plotting the heatmap with a larger figure size\n",
        "plt.figure(figsize=(200, 200))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", xticklabels=last_three_words, yticklabels=last_three_words)\n",
        "plt.title('Spearman Correlation Matrix')\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "vgq3iZa-LnUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGN6YSk9q71F"
      },
      "outputs": [],
      "source": [
        "def shift_feature(\n",
        "    df: pd.DataFrame,\n",
        "    conti_cols: List[str],\n",
        "    intervals: List[int],\n",
        ") -> List[pd.Series]:\n",
        "    \"\"\"\n",
        "    연속형 변수의 shift feature 생성\n",
        "    Args:\n",
        "        df (pd.DataFrame)\n",
        "        conti_cols (List[str]): continuous colnames\n",
        "        intervals (List[int]): shifted intervals\n",
        "    Return:\n",
        "        List[pd.Series]\n",
        "    \"\"\"\n",
        "    df_shift_dict = [\n",
        "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
        "        for conti_col in conti_cols\n",
        "        for interval in intervals\n",
        "    ]\n",
        "    return df_shift_dict\n",
        "\n",
        "# 최대 24시간의 shift 피쳐를 계산\n",
        "shift_list = shift_feature(\n",
        "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rNTm9gHq71G"
      },
      "outputs": [],
      "source": [
        "# concat 하여 df 에 할당\n",
        "\n",
        "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
        "\n",
        "# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체\n",
        "_target = df[\"target\"]\n",
        "# df = df.ffill().fillna(-999).assign(target = _target)\n",
        "\n",
        "# _type에 따라 train, test 분리\n",
        "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
        "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only the desired features\n",
        "features = ['volume_diff', 'buy_ratio', 'liquidation_diff', 'open_interest']\n",
        "train = train_df[features + ['target']]  # Assuming 'target' is the label column\n",
        "test = test_df[features]\n",
        "\n",
        "# Fill missing values with the mean of 4 values above and below\n",
        "def fill_missing_with_mean(df, features):\n",
        "    for feature in features:\n",
        "        df[feature] = df[feature].fillna(df[feature].rolling(4, min_periods=1).mean())\n",
        "        df[feature] = df[feature].fillna(df[feature].rolling(4, min_periods=1).mean().shift(-4))\n",
        "    return df\n",
        "\n",
        "train = fill_missing_with_mean(train, features)\n",
        "test = fill_missing_with_mean(test, features)\n",
        "\n",
        "# Split data into features and target\n",
        "X = train[features]\n",
        "y = train['target']\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Validate the model\n",
        "y_pred_val = xgb_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n')\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "\"\"\"train_dataset = lgb.Dataset(X_train, label=y_train)\n",
        "val_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 4,\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting': 'gbdt',\n",
        "    'num_leaves': 50,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.8,\n",
        "    'random_state': 42,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Train LightGBM model\n",
        "lgb_model = lgb.train(params, train_dataset, valid_sets=val_dataset, num_boost_round=100)\n",
        "\n",
        "y_pred = lgb_model.predict(X_val)\n",
        "y_pred_class = [list(x).index(max(x)) for x in y_pred]\n",
        "print(\"Validation accuracy:\", accuracy_score(y_val, y_pred_class))\"\"\"\n"
      ],
      "metadata": {
        "id": "4T8-roAO_lEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 데이터로 XGBoost 모델 학습\n",
        "xgb_model_full = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss'\n",
        ")\n",
        "\n",
        "x_train = train_df.drop([\"target\", \"ID\"], axis = 1)\n",
        "y_train_new = train_df[\"target\"].astype(int)\n",
        "\n",
        "xgb_model_full.fit(x_train, y_train_new)\n",
        "\n",
        "y_pred_val = xgb_model.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred_val)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print('Classification Report:\\n')\n",
        "print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "# Test 데이터 예측\n",
        "y_pred_test = xgb_model_full.predict(test_df.drop([\"target\", \"ID\"], axis = 1))\n",
        "\n",
        "# Create output file\n",
        "output = pd.DataFrame({'ID': test_df['ID'], 'target': y_pred_test})\n",
        "output.to_csv('output.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to output.csv\")"
      ],
      "metadata": {
        "id": "z9q_m2DnEwzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['target'].value_counts()"
      ],
      "metadata": {
        "id": "oWo_Iiiybzh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "_2gt-MLodPgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "puC0XvWJdP0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize W&B\n",
        "wandb.init(project=\"btc-prediction\")\n",
        "\n",
        "# Define your hyperparameter sweep configuration\n",
        "sweep_config = {\n",
        "    'method': 'grid',  # or 'grid'\n",
        "    'metric': {'name': 'accuracy', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'n_estimators': {'values': [50, 100, 150]},\n",
        "        'learning_rate': {'values': [0.01, 0.05, 0.1]},\n",
        "        'max_depth': {'values': [4, 6, 8]},\n",
        "        'subsample': {'values': [0.6, 0.8, 1.0]},\n",
        "        'colsample_bytree': {'values': [0.6, 0.8, 1.0]}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize a sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"btc-prediction\")\n",
        "\n",
        "# Define a function to train your model\n",
        "def train():\n",
        "    # Access the hyperparameters through wandb.config\n",
        "    config = wandb.config\n",
        "\n",
        "    # Initialize the model with current hyperparameters\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=config.n_estimators,\n",
        "        learning_rate=config.learning_rate,\n",
        "        max_depth=config.max_depth,\n",
        "        subsample=config.subsample,\n",
        "        colsample_bytree=config.colsample_bytree,\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(x_train, y_train_new, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions and evaluate\n",
        "    y_pred_val = xgb_model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred_val)\n",
        "\n",
        "    # Log the results to W&B\n",
        "    wandb.log({'accuracy': accuracy})\n",
        "\n",
        "    # Optionally, log additional metrics like the classification report\n",
        "    print(f'Validation Accuracy: {accuracy}')\n",
        "    print('Classification Report:\\n')\n",
        "    print(classification_report(y_val, y_pred_val))\n",
        "\n",
        "# Run the sweep agent\n",
        "wandb.agent(sweep_id, function=train)\n",
        "\n",
        "# Test 데이터 예측\n",
        "y_pred_test = xgb_model.predict(test_df.drop([\"target\", \"ID\"], axis = 1))\n",
        "\n",
        "# Create output file\n",
        "output = pd.DataFrame({'ID': test_df['ID'], 'target': y_pred_test})\n",
        "output.to_csv('output.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to output.csv\")"
      ],
      "metadata": {
        "id": "w8GTefHGdNSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}