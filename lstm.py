# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-XIUN3ZBxS5ml3bEcsiyMrlewyOaeoy

### Library Import
"""

import os
from typing import List, Dict
from tqdm import tqdm
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import random
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score
import lightgbm as lgb
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns

seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)

"""### Data Load"""

# 파일 호출
data_path: str = "../data"
train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, "train.csv")).assign(_type="train") # train 에는 _type = train
test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, "test.csv")).assign(_type="test") # test 에는 _type = test
submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, "test.csv")) # ID, target 열만 가진 데이터 미리 호출
initial_df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)

# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩
file_names: List[str] = [
    f for f in os.listdir(data_path) if f.startswith("HOURLY_") and f.endswith(".csv")
]

# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장
file_dict: Dict[str, pd.DataFrame] = {
    f.replace(".csv", ""): pd.read_csv(os.path.join(data_path, f)) for f in file_names
}

for _file_name, _df in tqdm(file_dict.items()):
    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경
    _rename_rule = {
        col: f"{_file_name.lower()}_{col.lower()}" if col != "datetime" else "ID"
        for col in _df.columns
    }
    _df = _df.rename(_rename_rule, axis=1)
    initial_df = initial_df.merge(_df, on="ID", how="left")

"""### 결측치 확인
받은 데이터상에서 결측치는 없어 보이나,
위 df는 2023.01.01~2024.04.26까지만의 데이터로 left join 하였으므로 이에 따른 결측치가 발생한다.

이때 train data에서는 결측치가 100%가 아니더라도 test data에서 100%의 결측치가 발생할 수 있음을 확인하였다

이는 곧 train data에서 사용하더라도 test data로 예측 시 사용할 수 없는 feature이므로, 해당 feature들을 제거하고 학습할 필요가 있다
"""

eda_train_df = initial_df.loc[initial_df["_type"] == "train"]
# 각 열에서 누락된 값의 수를 계산
missing_values = eda_train_df.isnull().sum()

# 누락된 값의 백분율 계산
missing_percentage = (missing_values / len(eda_train_df)) * 100

# 누락된 값 비율을 기준으로 열 정렬
sorted_missing_percentage = missing_percentage.sort_values(ascending=False)
sorted_missing_percentage

sorted_missing_percentage.value_counts().sort_values(ascending=False)
# sorted_missing_percentage[sorted_missing_percentage > 10].shape

eda_test_df = initial_df.loc[initial_df["_type"] == "test"]
# 각 열에서 누락된 값의 수를 계산
test_missing_values = eda_test_df.isnull().sum()

# 누락된 값의 백분율 계산
test_missing_percentage = (test_missing_values / len(eda_test_df)) * 100

# 누락된 값 비율을 기준으로 열 정렬
test_sorted_missing_percentage = test_missing_percentage.sort_values(ascending=False)
test_sorted_missing_percentage

test_sorted_missing_percentage.value_counts().sort_values(ascending=False)

print(test_sorted_missing_percentage[test_sorted_missing_percentage > 50])
print(test_sorted_missing_percentage[test_sorted_missing_percentage > 50].shape)

print(test_sorted_missing_percentage[test_sorted_missing_percentage == 0])
print(test_sorted_missing_percentage[test_sorted_missing_percentage == 0].shape)

for col in test_sorted_missing_percentage[test_sorted_missing_percentage > 50].index:
    if col not in missing_percentage[missing_percentage > 50].index:
        print(col)

for col in sorted_missing_percentage[sorted_missing_percentage > 50].index:
    if col not in test_missing_percentage[test_missing_percentage > 50].index:
        print(col)

"""**결론**


관측 결과 train에서 결측치가 100%고 test에서 결측치가 100%가 아닌 경우는 없으나,

test에서 결측치가 100%고 train에서 결측치가 100%가 아닌 feature가 target column을 제외하면 7개가 관측된다.

따라서 test data에서 결측치가 100%인 feature들만 제거해주고 학습하면 된다 (target column은 빼면 안된다!)
"""

# 아래 코드만 돌려서 누락된 값이 50% 이상인 열을 제거

eda_test_df = initial_df.loc[initial_df["_type"] == "test"]
# 각 열에서 누락된 값의 수를 계산
test_missing_values = eda_test_df.isnull().sum()

# 누락된 값의 백분율 계산
test_missing_percentage = (test_missing_values / len(eda_test_df)) * 100

# 누락된 값 비율을 기준으로 열 정렬
test_sorted_missing_percentage = test_missing_percentage.sort_values(ascending=False)
test_sorted_missing_percentage

columns_to_drop = test_sorted_missing_percentage[test_sorted_missing_percentage > 50].index
columns_to_drop = columns_to_drop.difference(['target'])  # 'target' 열을 제외

initial_df = initial_df.drop(columns=columns_to_drop)
# initial_df.tail(50)

initial_df.columns.tolist()

"""### EDA (Explanatory Data Analysis)"""

eda_df = initial_df.filter(like="open-interest")

eda_df.reset_index(inplace=True)
df_melted = eda_df.melt(id_vars=['index'], var_name='Column', value_name='Value')

plt.figure(figsize=(30, 6))
sns.lineplot(x='index', y='Value', hue='Column', data=df_melted)

# 그래프 제목과 축 레이블 설정
plt.title('Line Plot of Different Columns')
plt.xlabel('Index (Number)')
plt.ylabel('Value')

plt.show()

"""**결론**
: hourly_market-data_open-interest_all_exchange_all_symbol_open_interest 만 쓰기
"""

eda_df = initial_df.filter(like='btc_usd_close', axis=1)

eda_df.reset_index(inplace=True)
df_melted = eda_df.melt(id_vars=['index'], var_name='Column', value_name='Value')

eda_df.head()

df_melted.head()

plt.figure(figsize=(30, 6))
sns.lineplot(x='index', y='Value', hue='Column', data=df_melted[:30*24])

# 그래프 제목과 축 레이블 설정
plt.title('Volume and Close Price Line Plot')
plt.xlabel('Index (Number)')
plt.ylabel('Value')

plt.show()

"""### Feature Engineering"""

# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함
cols_dict: Dict[str, str] = {
    "ID": "ID",
    "target": "target",
    "_type": "_type",
    "hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations": "long_liquidations",
    "hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations": "short_liquidations",
    "hourly_market-data_open-interest_all_exchange_all_symbol_open_interest": "open_interest",
    "hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio": "buy_sell_ratio",
    "hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close": "close",
    "hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume": "buy_volume",
    "hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume": "sell_volume",
    "hourly_network-data_addresses-count_addresses_count_active": "active_count",
    "hourly_network-data_addresses-count_addresses_count_receiver": "receiver_count",

}

conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in ["ID", "target", "_type", "close"]]
df = initial_df[cols_dict.keys()].rename(cols_dict, axis=1)
df.shape

shift_cols = {"sell_volume": 1, "receiver_count": 7, "active_count": 7}
def shift_feature(
    df: pd.DataFrame,
    shift_cols: Dict[str, int],
) -> List[pd.Series]:
    """
    연속형 변수의 shift feature 생성
    Args:
        df (pd.DataFrame)
        conti_cols (List[str]): continuous colnames
        intervals (List[int]): shifted intervals
    Return:
        List[pd.Series]
    """
    df_shift_dict = [
        df[conti_col].shift(interval).rename(f"{conti_col}_{interval}")
        for conti_col, interval in shift_cols.items()
    ]
    return df_shift_dict

# 최대 8시간의 shift 피쳐를 계산
shift_list = shift_feature(
    df=df, shift_cols=shift_cols
)

rolling_cols = [_ for _ in conti_cols if _  not in ["open_interest", "long_liquidations", "short_liquidations"]]
def rolling_feature(df: pd.DataFrame, conti_cols: List[str], window_size: int) -> List[pd.Series]:
    """
    연속형 변수의 rolling feature 생성
    Args:
        df (pd.DataFrame)
        conti_cols (List[str]): continuous colnames
        window_size (int): rolling window size
    Return:
        List[pd.Series]
    """
    df_rolling_list = [
        df[conti_col].rolling(window=window_size).mean().rename(f"{conti_col}_rolling_mean_{window_size}")
        for conti_col in conti_cols
    ]
    return df_rolling_list

rolling_list = rolling_feature(df=df, conti_cols=conti_cols, window_size=24)

# concat 하여 df 에 할당
df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)
# df = pd.concat([df, pd.concat(rolling_list, axis=1)], axis=1)
df = df.drop(["active_count", "receiver_count"], axis=1)

# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체
_target = df["target"]
df = df.ffill().fillna(-999).assign(target = _target)

# _type에 따라 train, test 분리
train_df = df.loc[df["_type"]=="train"].drop(columns=["_type"])
test_df = df.loc[df["_type"]=="test"].drop(columns=["_type"])

feature_cols_list = [_ for _ in df.columns if _ not in ["ID", "target", "_type", "close"]]

df.columns.tolist()

"""### 계산한 class와 실제 class 비교"""

classify_weight = 0.1
def classify_change(prev_value, curr_value):
    change_ratio = (curr_value - prev_value) / abs(curr_value)
    if np.isnan(change_ratio) or pd.isna(change_ratio):
        return 2
    if change_ratio > classify_weight:
        return 3  # 큰 상승
    elif change_ratio > 0:
        return 2  # 약간 상승
    elif change_ratio > -classify_weight:
        return 1  # 약간 하락
    else:
        return 0  # 큰 하락

"""### Model Training"""

# 데이터 로더: 데이터프레임을 받아서 train/eval 모드에 따라 (sequence, target) numpy 배열을 반환
def data_loader(df, mode, input_scaler, output_scaler, sequence_length):
    features = df[feature_cols_list].values
    target = df["close"].values
    ground_truth = df["target"].values
    if mode == "train":
        features_scaled = input_scaler.fit_transform(features)
        target_scaled = output_scaler.fit_transform(target.reshape(-1, 1))
    else:
        features_scaled = input_scaler.transform(features)
        target_scaled = output_scaler.transform(target.reshape(-1, 1))

    # X: (N, sequence_length, num_features), Y: (N,)
    X, Y = [], []
    for i in range(len(features_scaled)- sequence_length):
        X.append(features_scaled[i:i+sequence_length])
        Y.append(target_scaled[i+sequence_length])

    print(f"Number of sequences: {len(X)}")

    return np.array(X), np.array(Y), np.array(ground_truth)

train_seq_len = 8

# scaler 생성
input_scaler = MinMaxScaler(feature_range=(0, 1))
output_scaler = MinMaxScaler(feature_range=(0, 1))

# train 데이터 로드하기
X_train, y_train, output_train = data_loader(train_df, mode="train", input_scaler=input_scaler, output_scaler=output_scaler, sequence_length=train_seq_len)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).squeeze()
output_train_df = pd.DataFrame(output_train, columns=["class"])
direction_train_tensor = np.where((output_train == 0) | (output_train == 1), 0, 1)
direction_train_tensor = torch.tensor(direction_train_tensor, dtype=torch.float32)
direction_train_tensor = direction_train_tensor[train_seq_len:]
print(X_train_tensor.shape)
print(y_train_tensor.shape)

def model_prediction(model, X, output_scaler):
    model.eval()
    prediction, _ = model(X)
    prediction = output_scaler.inverse_transform(prediction.detach().numpy())
    prediction_df = pd.DataFrame(prediction)
    print(prediction_df.describe())
    return prediction

def classify_change(prev_value, curr_value):
    change_ratio = (curr_value - prev_value) / abs(curr_value)
    if np.isnan(change_ratio) or pd.isna(change_ratio):
        return 2
    if change_ratio > classify_weight:
        return 3  # 큰 상승
    elif change_ratio > 0:
        return 2  # 약간 상승
    elif change_ratio > -classify_weight:
        return 1  # 약간 하락
    else:
        return 0  # 큰 하락

def model_accuracy(model, X, output, output_scaler, doesPrint=False):
    # 예측하기
    prediction = model_prediction(model, X=X, output_scaler=output_scaler)
    prediction_df = pd.DataFrame(prediction)

    # df[0] 데이터 가져오기
    first_value = prediction_df.iloc[0]
    # df[0] 데이터를 8줄로 쌓기
    new_rows = pd.DataFrame([first_value] * train_seq_len, columns=prediction_df.columns)
    # 기존 df 앞에 new_rows 추가
    prediction_df = pd.concat([new_rows, prediction_df], ignore_index=True)

    prediction_df.columns = ["predicted_close"]

    # predicted_close로 predicted_class 분류하기
    prediction_df["next_predicted_close"] = prediction_df["predicted_close"].shift(-1)
    prediction_df['predicted_class'] = prediction_df.apply(lambda x: classify_change(x['predicted_close'], x['next_predicted_close']), axis=1)
    final_prediction_df = pd.merge(prediction_df, output, left_index=True, right_index=True, how='inner')
    if doesPrint:
        print(final_prediction_df[["class", "predicted_class"]].value_counts().sort_index())

    # accuracy 계산하기
    wrong_prediction = final_prediction_df[final_prediction_df["class"]!=final_prediction_df["predicted_class"]]
    accuracy = 1 - wrong_prediction.shape[0] / output.shape[0]
    return final_prediction_df, accuracy

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.5):
        super(LSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)

        # 회귀 (가격 예측) 출력을 위한 선형 레이어
        self.price_fc = nn.Linear(hidden_size, 1)  # 코인 가격 출력

        # 이진 분류 (등락 예측) 출력을 위한 선형 레이어
        self.direction_fc = nn.Linear(hidden_size, 1)  # 상승/하락 여부 출력
        self.sigmoid = nn.Sigmoid()  # 등락 예측을 위한 활성화 함수

    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)
        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)

        lstm_out, _ = self.lstm(x, (h0, c0))
        lstm_out = lstm_out[:, -1, :] # 마지막 시퀀스의 결과만 사용
        price_out = self.price_fc(lstm_out)
        direction_out = self.direction_fc(lstm_out)
        direction_out = self.sigmoid(direction_out)
        # direction_out = self.sigmoid(price_out)

        return price_out, direction_out

input_size = len(feature_cols_list)
hidden_size = 100
output_size = 1
num_layers = 2
# 0.4605, 0.5393
# weights = [100.0, 0.5393, 0.4605, 100.0]  # 클래스 불균형을 위한 가중치

model = LSTM(input_size, hidden_size, output_size, num_layers)
mseloss = nn.MSELoss()
bceloss = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
total_epoch = 0

# 모델 학습

initial_num_epochs = 2000

# for param_group in optimizer.param_groups:
#     param_group['lr'] = 0.005
for epoch in range(initial_num_epochs):

    model.train()
    optimizer.zero_grad()
    predicted_prices, predicted_directions = model(X_train_tensor)
    # if (epoch + 1) % 10 == 0:
    #     output_original = output_scaler.inverse_transform(output.detach().unsqueeze(1).numpy())
    #     output_df = pd.DataFrame(output_original)
    #     print(output_df.describe())

    predicted_prices = predicted_prices.squeeze()
    predicted_directions = predicted_directions.squeeze()
    mse = mseloss(predicted_prices, y_train_tensor)
    bce = bceloss(predicted_directions, direction_train_tensor)
    # loss = mse * 0.3 + bce * 0.7
    loss = bce*0.3 + mse * 1.2
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        prediction_df, accuracy = model_accuracy(model, X_train_tensor, output_train_df, output_scaler=output_scaler)
        print(f'epoch {epoch+total_epoch} : mseloss {mse.item()}, bceloss {bce.item()}, accuracy {accuracy}')
total_epoch+=initial_num_epochs

"""### Model Evaluation (with Train data)"""

prediction_df, accuracy = model_accuracy(model, X_train_tensor, output_train_df, output_scaler=output_scaler, doesPrint=True)
print(accuracy)

plt.figure(figsize=(50, 6))

def eda_result(offset = 0, length = len(y_train)):
    # prediction_df["class"] = prediction_df["class"].shift()
    eda_df = prediction_df[["predicted_class", "class"]][offset:min(len(y_train), offset+length)]
    eda_df.plot(kind="line", figsize=(50, 6))
    # plt.scatter(range(len(eda_y)), y=eda_y, color='#aaaaaa', s=10)
    # plt.scatter(range(len(eda_p)), y=eda_p,s=1)

eda_result(0*24, 30*24)

# 그래프 제목과 축 레이블 설정
plt.title(f'accuracy {accuracy}  |  total_epoch: {total_epoch}, hidden_size: {hidden_size}, sequence_length: {train_seq_len}, lr {optimizer.param_groups[0]["lr"]} with classifying weight {classify_weight}')
plt.xlabel('Index (Number)')
plt.ylabel('Value')

plt.show()

eda_df = prediction_df[["predicted_close"]]
eda_df.reset_index(inplace=True)
df_melted = eda_df.melt(id_vars=['index'], var_name='Column', value_name='Value')

plt.figure(figsize=(30, 6))
sns.lineplot(x='index', y='Value', hue='Column', data=df_melted[:30*24])

# 그래프 제목과 축 레이블 설정
plt.title(f'Volume and Close Price Line Plot with rolling 8 {total_epoch}')
plt.xlabel('Index (Number)')
plt.ylabel('Value')

plt.show()

"""### Inference"""

X_test, y_test, output_test = data_loader(test_df, mode="test", input_scaler=input_scaler, output_scaler=output_scaler, sequence_length=train_seq_len)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
output_test_df = pd.DataFrame(output_test, columns=["class"])

test_prediction_df, _ = model_accuracy(model, X_test_tensor, output_test_df, output_scaler=output_scaler)

submission_df = test_df[["ID", "target"]].copy()
submission_df["target"] = test_prediction_df["predicted_class"].values

print(test_prediction_df[["predicted_class"]].value_counts().sort_index() / test_prediction_df.shape[0])

"""### Output File Save"""

# output file 할당후 save
submission_df.to_csv(f"output/output_{output_file_idx}_{accuracy:.4f}.csv", index=False)